services:

  haystack-api:
    image: "ghcr.io/loopx-ai/haystack:gpu"
    volumes:
      # - ./rest_api/rest_api:/opt/rest_api
      - ./rest_api/rest_api/pipeline:/opt/pipelines
      - hscache_data:/home/haystackd/.cache
      - hsnltk_data:/home/haystackd/nltk_data
      - hsfile_data:/opt/file-upload
    ports:
      - 9210:8000
    restart: on-failure
    # command: uvicorn rest_api.application:app --reload --host 0.0.0.0
    runtime: nvidia
    environment:
      - LOG_LEVEL=INFO
      - NVIDIA_VISIBLE_DEVICES=all
      - DOCUMENTSTORE_PARAMS_HOST=elasticsearch
      - PIPELINE_YAML_PATH=/opt/pipelines/pipelines_aisear.haystack-pipeline.yml
      - TOKENIZERS_PARALLELISM=false
      # Uncomment the following line to customise how much time (in seconds) a worker can spend serving a request
      # before it times out. This should include the time required to cache the models and setup the pipelines.

      # - GUNICORN_CMD_ARGS="--timeout=3000"
    depends_on:
      elasticsearch:
        condition: service_healthy

  elasticsearch:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.6"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    restart: on-failure
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
    healthcheck:
        test: curl --fail http://localhost:9200/_cat/health || exit 1
        interval: 10s
        timeout: 1s
        retries: 10

volumes:
  hsfile_data:
    driver: local
  hscache_data:
    driver: local
  hsnltk_data:
    driver: local
  esdata:
    driver: local